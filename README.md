# Kaggle Competitions Documentation

This repo contains coded solutions and configurations for entered [Kaggle competitions](https://www.kaggle.com/oislen). The repository is divided up into the following three subdirectories:

1. [Competitions](https://github.com/oislen/Kaggle/tree/master/competitions)
    1. [Digit Recognizer](https://github.com/oislen/Kaggle/tree/master/competitions/Digit_Recognizer/scripts)
	2. [Disaster Tweets](https://github.com/oislen/Kaggle/tree/master/competitions/Disaster_Tweets/scripts)
	3. [House Prices](https://github.com/oislen/Kaggle/tree/master/competitions/HousePrices/scripts)
	4. [Predict Future Sales](https://github.com/oislen/Kaggle/tree/master/competitions/Predict_Future_Sales/scripts)
	5. [Titanic](https://github.com/oislen/Kaggle/tree/master/competitions/Titanic/scripts)
2. [Environments](https://github.com/oislen/Kaggle/tree/master/environments)
3. [Utilties](https://github.com/oislen/Kaggle/tree/master/utilities)

## Competitions 

### Digit Recognizer

#### Description

This subdirectory contains the code for the [digit recogniser competition](https://www.kaggle.com/c/digit-recognizer). The goal of the competition is to classify tens of thousands of handwritten images from the MNIST ("Modified National Institute of Standards and Technology") dataset. The simple dataset is often used many computer vision tasks and serves as a basis for benchmarking classification algorithms.

#### Methodology

The images are standardised to \[0, 1\] by dividing all pixal values by 255. Next the standardised images are reshaped to (28 x 28 x 1) dimensions for model input. Following this, augmented images are generated by flipping and shifting existing images. Finally, the data is split into training, validation and test sets. 
An CNN model is built using the LeNet architecture. The model is trained and validated using accuracy. The model also implements a reduced learning rate upon when learning rate begins to plateau. Model performance is summarised with a confusion matrix and classification errors are analysed.

### Disaster Tweets

#### Description

This competition directory contains the logic and code for the [Natural Language Processing with Disaster Tweets competition](https://www.kaggle.com/c/nlp-getting-started). The object of this competition is to classify whether the contents of a tweet are about a disaster or not. The dataset contains over 10k tweets. This particular challenge is perfect for getting started with Natural Language Procesisng.

#### Methodology

First the data is processed and cleaned by standardising the tweets to lower case and removing url links, html tags, emojis and punctuation. Next a corpus of tweets is generated by transforming the normalised tweets into a lists of tweets split up into words. Pre-trained GloVe word vector embeddings are then used to translate each tweet in the corpus to a sequence of word vectors.
After the data has been processed and is ready for modelling, a Recurrent Neural Network was created, trained and validated using Keras. The model was trained with binary cross-entropy loss and validated using accuracy.

### House Prices

#### Description

The HousePrices directory holds all of the code for the [House Prices competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques). The purpose of this competition is to predict the final price of residential homes in Ames, Iowa using 79 explanatory variables such as basement ceiling height and distance to railroad. The data has plenty of scope for feature engineering and advanced regression techniques.

#### Methodology

The data is initially cleaned by filling in zeros, modes and medians where appropriate. Afterwards, categorical variables with a natural order are mapped to ordinal variables, and then dummy encoded. Further feature engineering is performed by removing outliers, deriving interactiong terms and transforming skewed attributes with a box-cox power transaformation. Finally, a final set of predictors are selected using feature importance ranking from a random forest model. A weighted ensemble model containing a xgboost model, a lgboost model and stacked sklearn model (Elastic Nets, Gradient Boosting Regressor and Kernel Ridge Regressor) was then trained, tested and used for predicting the final test file.

### Predict Future Sales

#### Description

The code contained within this folder is responable for the [Predict Future Sales competition](https://www.kaggle.com/c/competitive-data-science-predict-future-sales). This challenge serves as the final project for the Coursera course, ["How to win a data science competition"](https://www.coursera.org/learn/competitive-data-science). The competition uses a challenging time-series dataset of daily sales data proved by [1C Company](https://1c.ru/eng/title.htm), one of the largest Russian software firms. The goal of the competition is to predict the total sales for every product and store in the next month.

#### Methodology

The raw data is first processed for modelling. This initially involves cleaning messy string data and down casting data types. Afterwards, the data is aggregated to shop and item level, and missing item values are back filled. Next, mean and totla shift attributes are generated for each of the past four months, and categories are mean encoded using the target column.
Once the data has been cleaned, a random forest model and a gradient boosting model were fitted, validated and trainined using cross-validation and hyper parameter tuning. The performance of these predictions was accessed using rmse and prediction vs observed plots. The predictions of these two models were then passed to a second layer poisson model for generating the final submission kaggle file.

### Titanic <a name="titanic"></a>

#### Description

The Titanic folder contains the code for the [Titanic - Machine Learning from Disaster competition](https://www.kaggle.com/c/titanic). The aim of the competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck. The dataset used in the competition holds passenger information such as name, age, gender and soci-economic class.

#### Methodology

Firstly, the raw data is cleaned and engineered for modelling. This includes, filling in missing values, dummy encoding cateogircal variables, transforming and binning numeric variables into ordinal variables and engineering new features. All resulting predictors are standardised to the interval \[0, 1\] prior to modelling. 
Once the data has been cleaned, a variety of tree based sklearn models such as random forests and gradient boosted decision trees are fitted and validated. The minority class of the target variable is up sampled using SMOTE. Hyper parameter tuning is applied over 10 fold cross validation using accuracy as an evaluation metric. Once the optimal model has been determined, it is validated using a holdout set by plotting a ROC curve and calculating various classification metrics such as recall, precision and F1. After the optimal model has been validate with the holdout set, it is pickled and predictions are made for the test set. Finally, a majority vote ensemble classifier is generated using all of the optimal tree based sklearn models and their test set predictions.

## Environments

The environments subdirectory contains .yml files, .sh shell scripts and .bat batch scripts for installing previously configured conda environments for competition use. 
This directory also contains custom configurations for setting up an EC2 instance on AWS for running the above kaggle competitions. This includes  configuring linux permissions and directories, enabling a tigervnc gui and installing anaconda.

## Utilities

This folder contains utility functions for interacting with the Kaggle Platform through Python using the [Kaggle API](https://github.com/Kaggle/kaggle-api). These utility functions are applicable to all competitions and can be used for:

- Downloading the relevant competition data from Kaggle 
- Submitting competition predictions for evaluation on the Kaggle Platform
- Returning the results and performance of previous competition submissions.

This directory also contains some generic analytical functions for processing, modelling and visualising data that are also shared and reused among the kaggle competitions.