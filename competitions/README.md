## Competitions 

1. [Digit Recognizer](#digit-recognizer)
2. [House Prices](#house-prices-advanced-regression-techniques)
3. [Predict Future Sales](#competitive-data-science)
4. [Titanic](#titanic)

### Digit Recognizer <a name="digit-recognizer"></a>

#### Description

This subdirectory contains the code for the [digit recogniser competition](https://www.kaggle.com/c/digit-recognizer). The goal of the competition is to classify tens of thousands of handwritten images from the MNIST ("Modified National Institute of Standards and Technology") dataset. The simple dataset is often used many computer vision tasks and serves as a basis for benchmarking classification algorithms.

#### Methodology

The images are standardised to \[0, 1\] by dividing all pixal values by 255. Next the standardised images are reshaped to (28 x 28 x 1) dimensions for model input. Following this, augmented images are generated by flipping and shifting existing images. Finally, the data is split into training, validation and test sets. 
An CNN model is built using the LeNet architecture. The model is trained and validated using accuracy. The model also implements a reduced learning rate upon when learning rate begins to plateau. Model performance is summarised with a confusion matrix and classification errors are analysed.

### House Prices <a name="house-prices-advanced-regression-techniques"></a>

#### Description

The HousePrices directory holds all of the code for the [House Prices competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques). The purpose of this competition is to predict the final price of residential homes in Ames, Iowa using 79 explanatory variables such as basement ceiling height and distance to railroad. The data has plenty of scope for feature engineering and advanced regression techniques.

#### Methodology

The data is initially cleaned by filling in zeros, modes and medians where appropriate. Afterwards, categorical variables with a natural order are mapped to ordinal variables, and then dummy encoded. Further feature engineering is performed by removing outliers, deriving interactiong terms and transforming skewed attributes with a box-cox power transaformation. Finally, a final set of predictors are selected using feature importance ranking from a random forest model. A weighted ensemble model containing a xgboost model, a lgboost model and stacked sklearn model (Elastic Nets, Gradient Boosting Regressor and Kernel Ridge Regressor) was then trained, tested and used for predicting the final test file.

### Predict Future Sales <a name="competitive-data-science"></a>

The code contained within this folder is responable for the [Predict Future Sales competition](https://www.kaggle.com/c/competitive-data-science-predict-future-sales). This challenge serves as the final project for the Coursera course, ["How to win a data science competition"](https://www.coursera.org/learn/competitive-data-science). The competition uses a challenging time-series dataset of daily sales data proved by [1C Company](https://1c.ru/eng/title.htm), one of the largest Russian software firms. The goal of the competition is to predict the total sales for every product and store in the next month.

### Titanic <a name="titanic"></a>

#### Description

The Titanic folder contains the code for the [Titanic - Machine Learning from Disaster competition](https://www.kaggle.com/c/titanic). The aim of the competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck. The dataset used in the competition holds passenger information such as name, age, gender and soci-economic class.

#### Methodology

Firstly, the raw data is cleaned and engineered for modelling. This includes, filling in missing values, dummy encoding cateogircal variables, transforming and binning numeric variables into ordinal variables and engineering new features. All resulting predictors are standardised to the interval \[0, 1\] prior to modelling. 
Once the data has been cleaned, a variety of tree based sklearn models such as random forests and gradient boosted decision trees are fitted and validated. The minority class of the target variable is up sampled using SMOTE. Hyper parameter tuning is applied over 10 fold cross validation using accuracy as an evaluation metric. Once the optimal model has been determined, it is validated using a holdout set by plotting a ROC curve and calculating various classification metrics such as recall, precision and F1. After the optimal model has been validate with the holdout set, it is pickled and predictions are made for the test set. Finally, a majority vote ensemble classifier is generated using all of the optimal tree based sklearn models and their test set predictions.
